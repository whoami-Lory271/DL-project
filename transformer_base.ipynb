{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b74bb0183bd649b5bd67240725cbcf10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35d793d7aeb94f5aa097c6625180948a",
              "IPY_MODEL_4a4ad6ead22845f3948c8d03b4368f38",
              "IPY_MODEL_780c361b90e74e31bda65e3c148edaff"
            ],
            "layout": "IPY_MODEL_2994496e19744e42bf1f2291b83580c5"
          }
        },
        "35d793d7aeb94f5aa097c6625180948a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c54da3b43bda49c5b161541f2aaeb719",
            "placeholder": "​",
            "style": "IPY_MODEL_f8f34073224a441e94734bfeb4ce04d7",
            "value": "Epoch 0:   0%"
          }
        },
        "4a4ad6ead22845f3948c8d03b4368f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_718978e038894727ba3cc5d72548d0a1",
            "max": 62500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27fbdf5b292d42d796f31678039106b9",
            "value": 200
          }
        },
        "780c361b90e74e31bda65e3c148edaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_915d3972336744a4be17ac991da327ef",
            "placeholder": "​",
            "style": "IPY_MODEL_cdfce29bc659443fa92e06b89eb37715",
            "value": " 200/62500 [04:59&lt;25:55:36,  1.50s/it, loss=1.23, v_num=3, train_loss_step=1.750]"
          }
        },
        "2994496e19744e42bf1f2291b83580c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c54da3b43bda49c5b161541f2aaeb719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8f34073224a441e94734bfeb4ce04d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "718978e038894727ba3cc5d72548d0a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27fbdf5b292d42d796f31678039106b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "915d3972336744a4be17ac991da327ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfce29bc659443fa92e06b89eb37715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/DL-project/blob/main/transformer_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install library import "
      ],
      "metadata": {
        "id": "EYE_g4aS0itt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install torchmetrics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7IXEB390pTj",
        "outputId": "c16f0abe-5cc0-444d-bd83-b0476e20e5ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import pickle\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar"
      ],
      "metadata": {
        "id": "6r_zVhYLKhqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "egcxUIj5C4dr",
        "outputId": "bea4c834-0863-44fc-ab7c-77f3eb88999d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function"
      ],
      "metadata": {
        "id": "ojG0bbT6C-JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_padding_mask(X):\n",
        "  pad = X == 0\n",
        "  padding_mask = pad.repeat(1,1,X.shape[1]).reshape((X.shape[0],X.shape[1],X.shape[1]))\n",
        "  padding_mask[pad] = True\n",
        "  return padding_mask"
      ],
      "metadata": {
        "id": "oZ4yDtvtGMSC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding(encode, vocab, max_len):\n",
        "    enc = np.array(encode)\n",
        "    encoded = np.zeros(max_len + 2)\n",
        "    for k in range(len(enc)):\n",
        "      encoded[k] = enc[k]\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "C4JVsg3-t_RN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "j2OkY9ii2qjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Deep learning\" #change if needed"
      ],
      "metadata": {
        "id": "DE4BW2JmUeSB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#take all the txt files\n",
        "easy_path = os.path.join(save_path,\"easy_pickle\")\n",
        "easy_texts_path=[str(path) for path in Path(easy_path).glob(\"*.txt\")]\n",
        "print(easy_texts_path)"
      ],
      "metadata": {
        "id": "5pVI-CFneMDR",
        "outputId": "f081cabd-67db-42c8-d5ad-7744f8bce176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/easy_pickle/algebra__linear_1d_easy.txt', '/content/drive/MyDrive/Deep learning/easy_pickle/calculus__differentiate_easy.txt', '/content/drive/MyDrive/Deep learning/easy_pickle/probability__swr_p_level_set_easy.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "medium_path= os.path.join(save_path,\"medium_pickle\")\n",
        "medium_texts_path=[str(path) for path in Path(medium_path).glob(\"*.txt\")]\n",
        "print(medium_texts_path)"
      ],
      "metadata": {
        "id": "6lVIhd7-eMzJ",
        "outputId": "34c20e9a-a6b1-4d6b-8b90-d6daed2e18e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/medium_pickle/algebra__linear_1d_medium.txt', '/content/drive/MyDrive/Deep learning/medium_pickle/calculus__differentiate_medium.txt', '/content/drive/MyDrive/Deep learning/medium_pickle/probability__swr_p_level_set_medium.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hard_path= os.path.join(save_path,\"hard_pickle\")\n",
        "hard_texts_path=[str(path) for path in Path(hard_path).glob(\"*.txt\")]\n",
        "print(hard_texts_path)"
      ],
      "metadata": {
        "id": "z7pos2G7eMsR",
        "outputId": "2dfc1710-08f9-49ae-c0c6-072343b9e096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/hard_pickle/algebra__linear_1d_hard.txt', '/content/drive/MyDrive/Deep learning/hard_pickle/calculus__differentiate_hard.txt', '/content/drive/MyDrive/Deep learning/hard_pickle/probability__swr_p_level_set_hard.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary_from_directory(sentences_path):\n",
        "  vocabulary = {}\n",
        "  vocabulary['<sos>'] = 1\n",
        "  vocabulary['<eos>'] = 2\n",
        "  vocabulary['<pad>'] = 0\n",
        "  index = 3\n",
        "  conta = 0\n",
        "  for file in sentences_path:\n",
        "    with open(file, 'r') as file_in:\n",
        "      text = file_in.read()\n",
        "      sentences = text.split('\\n')\n",
        "      sentences_new = sentences[:-1] #remove last element because it's empty\n",
        "      print(\"num sentences\", len(sentences_new))\n",
        "      for s in sentences_new:\n",
        "        conta +=1\n",
        "        for t in s:\n",
        "         if t not in vocabulary:\n",
        "            vocabulary[t] = index\n",
        "            index += 1\n",
        "      file_in.close()\n",
        "  return vocabulary, conta"
      ],
      "metadata": {
        "id": "NenOz7Bnz1gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary, conta = create_vocabulary_from_directory(ft_texts_path)\n",
        "# print(conta)"
      ],
      "metadata": {
        "id": "gw7vYDuiy5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #create the pickle\n",
        "# outfile = open(os.path.join(save_path,\"vocabs\"), 'wb')\n",
        "# pickle.dump(vocabulary, outfile)\n",
        "# outfile.close()"
      ],
      "metadata": {
        "id": "hozVF8z6KqDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_from_sentence(vocab, sentence):\n",
        "  enc= [1]\n",
        "  for token in sentence:\n",
        "    enc.append(vocab[token])\n",
        "  enc.append(2)\n",
        "  return enc"
      ],
      "metadata": {
        "id": "yE-32CsOX3en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pickles(vocabulary, sentences_paths, column_names=[\"Encoding\"], path_quest=\"/content/drive/MyDrive/Deep learning/questions\", path_answ=\"/content/drive/MyDrive/Deep learning/answers\"):\n",
        "  quest = []\n",
        "  answ = []\n",
        "  out_file_quest = open(path_quest, 'wb')\n",
        "  out_file_answ = open(path_answ, 'wb')\n",
        "  for file in sentences_paths:\n",
        "    with open(file, 'r') as file_in:\n",
        "      text = file_in.read()\n",
        "      sentences = text.split('\\n')\n",
        "      sentences = sentences[:-1]\n",
        "      print(\"num sentences\", len(sentences))\n",
        "      for i, sentence in enumerate(sentences):\n",
        "          enc = encoding_from_sentence(vocabulary, sentence)\n",
        "          if i % 2 == 0: #question\n",
        "            #quest.append({\"Sentence\": sentence, \"Encoding\": enc})\n",
        "            quest.append({\"Encoding\": enc})\n",
        "          elif i % 2 == 1: #answer\n",
        "            #answ.append({\"Sentence\": sentence, \"Encoding\": enc})\n",
        "            answ.append({\"Encoding\": enc})\n",
        "  pickle.dump(quest, out_file_quest)\n",
        "  pickle.dump(answ, out_file_answ)\n",
        "  out_file_quest.close()\n",
        "  out_file_answ.close()\n",
        "  return path_quest, path_answ"
      ],
      "metadata": {
        "id": "VJbw05t8Mjdx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "C8mQUo-4MPvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the vocabulary\n",
        "infile = open(os.path.join(save_path,'vocabs'),'rb')\n",
        "vocab = pickle.load(infile)"
      ],
      "metadata": {
        "id": "eJKBOht4EPHd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "infile = open(os.path.join(easy_path,\"questions\"),'rb')\n",
        "quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "infile = open(os.path.join(easy_path,\"answers\"),'rb')\n",
        "answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "id": "JoXn82htefTr",
        "outputId": "c7086066-d4df-4bff-9d95-8784b459db87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999998\n",
            "1999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path_quest, path_answ = create_pickles(vocab, ft_texts_path)"
      ],
      "metadata": {
        "id": "TCFq3MbYYrbv",
        "outputId": "db033c61-66ce-4e40-939c-10e9cde54c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ce4c295c6515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpath_quest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_answ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_pickles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_texts_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-7cb4dd80e1e1>\u001b[0m in \u001b[0;36mcreate_pickles\u001b[0;34m(vocabulary, sentences_paths, column_names, path_quest, path_answ)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mquest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mansw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mout_file_quest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_quest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mout_file_answ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_answ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content/drive/MyDrive/Deep learning/questions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "infile = open(os.path.join(save_path,\"questions\"),'rb')\n",
        "quest = pickle.load(infile)\n",
        "\n",
        "#unpickle the answers\n",
        "infile = open(os.path.join(save_path,\"answers\"),'rb')\n",
        "answ = pickle.load(infile)"
      ],
      "metadata": {
        "id": "t8e47VRR05pB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MathematicsDataset(Dataset):\n",
        "  def __init__(self, quest, answ, vocab , max_len_quest=160, max_len_answ=30):\n",
        "    self.quest = quest\n",
        "    self.answ = answ\n",
        "    self.vocab = vocab\n",
        "    self.max_len_quest = max_len_quest\n",
        "    self.max_len_answ = max_len_answ\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.quest)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    assert(idx  < len(self.quest)) #indices should start from 0 to len - 1 (there are 5999994 elements)\n",
        "    question = self.quest[idx][\"Encoding\"]\n",
        "    encoding1 = torch.from_numpy(create_padding(question, self.vocab, self.max_len_quest))\n",
        "    encoding1 = encoding1.type(torch.int64)\n",
        "    #self.sample[\"Question\"] = encoding\n",
        "      \n",
        "    answer = self.answ[idx][\"Encoding\"]\n",
        "    encoding2 = torch.from_numpy(create_padding(answer, self.vocab, self.max_len_answ))\n",
        "    encoding2 = encoding2.type(torch.int64)\n",
        "    #self.sample[\"Answer\"] = encoding\n",
        "      \n",
        "    return encoding1, encoding2 "
      ],
      "metadata": {
        "id": "bCskAS9LMNTT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlMathematicsDataset(pl.LightningDataModule):\n",
        "    def __init__(self, train_quest, train_answ, vocab , test_quest, test_answ, max_len_quest=160, max_len_answ=30, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.train_quest = train_quest\n",
        "        self.train_answ = train_answ\n",
        "        self.test_quest = test_quest\n",
        "        self.test_answ = test_answ\n",
        "        self.vocab = vocab\n",
        "        self.max_len_quest = max_len_quest\n",
        "        self.max_len_answ = max_len_answ\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = MathematicsDataset(self.train_quest, self.train_answ, self.vocab)\n",
        "\n",
        "    # def prepare_data(self):\n",
        "        # tok, load, ecc...\n",
        "        # MNIST(self.data_dir, train=True, download=True)\n",
        "        # MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_ds = MathematicsDataset(self.train_quest, self.train_answ, self.vocab)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_ds = MathematicsDataset(self.test_quest, self.test_answ, self.vocab)\n",
        "\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            self.predict_ds = MathematicsDataset(self.test_quest, self.test_answ, self.vocab)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle = True)\n",
        "\n",
        "    # def val_dataloader(self):\n",
        "    #     return DataLoader(self.mnist_val, batch_size=32)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle = True)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.predict_ds, batch_size=self.batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "dXLqCNPH1ZNQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dataset\n",
        "dataset = PlMathematicsDataset(quest, answ, vocab, None, None)"
      ],
      "metadata": {
        "id": "sAQcoCkOo44i"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Create dataloader\n",
        "# train_loader = DataLoader(dataset, 32, shuffle=True)\n",
        "# print(len(train_loader))"
      ],
      "metadata": {
        "id": "03Zzt0OmDgbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"data\", next(iter(train_loader)))\n",
        "# print(\"questions_shape \", next(iter(train_loader))[0].shape)\n",
        "\n",
        "# print(\"answers_shape \", next(iter(train_loader))[1].shape)"
      ],
      "metadata": {
        "id": "uWqFcYLND4ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quest = None\n",
        "answ = None"
      ],
      "metadata": {
        "id": "_BlgJWr8RR5F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Kf5YD-k_z5H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "  def __init__(self,vocabulary_size,embedding_dim):\n",
        "    super().__init__()\n",
        "    self.E = nn.Embedding(vocabulary_size,embedding_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.E(x)"
      ],
      "metadata": {
        "id": "C4i4vs8nLKNz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_embedding(batch_size,seq_length,emb_dim,pad_mask):\n",
        "  res = torch.zeros((batch_size,seq_length,emb_dim),dtype=torch.float32)\n",
        "  for pos in range(seq_length):\n",
        "    for i in range(emb_dim):\n",
        "      if i%2 == 0:\n",
        "        res[:,pos,i] = math.sin(pos/10000**(2*i/emb_dim))\n",
        "      else:\n",
        "        res[:,pos,i] = math.cos(pos/10000**(2*i/emb_dim))\n",
        "  res[pad_mask[:,0,:]] = 0\n",
        "  return res"
      ],
      "metadata": {
        "id": "B8E2HleXTgFz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPSelfAttention(nn.Module):\n",
        "  def __init__(self,dz,dk):\n",
        "    super().__init__()\n",
        "    self.dk = dk\n",
        "    self.Wq = nn.Linear(dz,dk)\n",
        "    self.Wk = nn.Linear(dz,dk)\n",
        "    self.Wv = nn.Linear(dz,dk)\n",
        "    self.Wr = nn.Linear(dz,dk)\n",
        "    self.Wo = nn.Linear(dk,dz)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    q = self.Wq(x)\n",
        "    r = self.Wr(x)\n",
        "    if enc == None:\n",
        "      k = self.Wk(x)\n",
        "      v = self.Wv(x)\n",
        "    else:\n",
        "      k = self.Wk(enc)\n",
        "      v = self.Wv(enc)\n",
        "    sc = torch.matmul(q,k.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "\n",
        "    if other_mask == None:\n",
        "      sc[padding_mask] = float('-inf')\n",
        "    else:\n",
        "      qmod = q.clone()\n",
        "      kmod = k.clone()\n",
        "      qmod[padding_mask[:,0,:]] = 0\n",
        "      kmod[other_mask[:,0,:]] = 0\n",
        "      sc = torch.matmul(qmod,kmod.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "      sc[sc == 0] = float('-inf')\n",
        "\n",
        "    if mask==True:\n",
        "      for i in range(sc.shape[1]):\n",
        "        sc[:,i,i+1:] = float('-inf')\n",
        "    score = torch.matmul(torch.nan_to_num(self.softmax(sc)),v)\n",
        "    return self.Wo(r * score)"
      ],
      "metadata": {
        "id": "v9OoIF1MgYIw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,dmodel,nhead,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.nhead = nhead\n",
        "    self.att_layers = nn.ModuleList([TPSelfAttention(dmodel,dmodel // nhead) for i in range(nhead)])\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    y = self.att_layers[0](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    for i in range(1,self.nhead):\n",
        "      y += self.att_layers[i](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    y = self.drop(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "wJS-HOR2mYjA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,df,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.W1 = nn.Linear(dmodel,df)\n",
        "    self.W2 = nn.Linear(df,dmodel)\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.W1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.W2(x)\n",
        "    x = self.drop(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q3a-LqjhyGER"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dmodel,df,nhead):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(dmodel)\n",
        "    self.mha = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,padding_mask):\n",
        "    x = self.norm(x)\n",
        "    z = self.mha(x,padding_mask)\n",
        "    z = self.norm1(x+z)\n",
        "    y = self.ffn(z)\n",
        "    return self.norm2(z+y)"
      ],
      "metadata": {
        "id": "mIPuC2RAypA5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dmodel,df,nhead):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(dmodel)\n",
        "    self.masked_mha = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.enc_dec_attention = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm3 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,enc,padding_mask,other_mask):\n",
        "    x = self.norm(x)\n",
        "    z1= self.masked_mha(x,padding_mask,mask=True)\n",
        "    z1 = self.norm1(x+z1)\n",
        "    z2= self.enc_dec_attention(z1,padding_mask,enc=enc,mask=False,other_mask=other_mask)\n",
        "    z2 = self.norm2(z1+z2)\n",
        "    y = self.ffn(z2)\n",
        "    return self.norm3(z2+y)"
      ],
      "metadata": {
        "id": "DLqzXKv6kRua"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Transformer(nn.Module):\n",
        "#   def __init__(self,voc_size,dmodel,df,nhead,nlayers,dropout=0.1):\n",
        "#     super().__init__()\n",
        "#     self.nlayers = nlayers\n",
        "#     self.embedding = EmbeddingLayer(voc_size,dmodel)\n",
        "#     self.encoders = nn.ModuleList([Encoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "#     self.decoders = nn.ModuleList([Decoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "#     self.drop1 = nn.Dropout(p=dropout)\n",
        "#     self.drop2 = nn.Dropout(p=dropout)\n",
        "\n",
        "#   def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "#     if encoding:\n",
        "#       emb = self.embedding(x)\n",
        "#       t = position_embedding(x.shape[0],x.shape[1],dmodel,in_padding_mask)\n",
        "#       x = emb + t\n",
        "#       x = self.drop1(x)\n",
        "#       for i in range(self.nlayers):\n",
        "#         x = self.encoders[i](x,in_padding_mask)\n",
        "\n",
        "#     emb = self.embedding(z)\n",
        "#     t = position_embedding(z.shape[0],z.shape[1],dmodel,out_padding_mask)\n",
        "#     z = emb + t\n",
        "#     z = self.drop2(z)\n",
        "#     for i in range(self.nlayers):\n",
        "#       z = self.decoders[i](z,x,out_padding_mask,in_padding_mask)\n",
        "    \n",
        "#     z = z @ self.embedding.E.weight.T\n",
        "\n",
        "#     return x,z"
      ],
      "metadata": {
        "id": "0lB1U2Ee4UCh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(pl.LightningModule):\n",
        "    def __init__(self,voc_size,dmodel,df,nhead,nlayers,dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.nlayers = nlayers\n",
        "      self.embedding = EmbeddingLayer(voc_size,dmodel)\n",
        "      self.encoders = nn.ModuleList([Encoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "      self.decoders = nn.ModuleList([Decoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "      self.drop1 = nn.Dropout(p=dropout)\n",
        "      self.drop2 = nn.Dropout(p=dropout)\n",
        "      self.dmodel = dmodel\n",
        "      self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes = voc_size)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        input, teacher = batch\n",
        "        in_pad_mask = get_padding_mask(input)\n",
        "        out_pad_mask = get_padding_mask(teacher)\n",
        "        x,output = tran(input,teacher,in_pad_mask,out_pad_mask)\n",
        "        pred = output[:,:-1,:].reshape(output.shape[0]*(output.shape[1] - 1),output.shape[2])\n",
        "        target = teacher[:,1:].reshape(teacher.shape[0]*(teacher.shape[1] - 1))\n",
        "        l = nn.CrossEntropyLoss()\n",
        "        loss = l(pred,target)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(tran.parameters(),lr=1e-04, betas=(0.9, 0.98), eps=1e-09)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x,y = batch\n",
        "        enc = True\n",
        "        trg = y.clone()\n",
        "        input_pad_mask = get_padding_mask(x)\n",
        "        for i in range(1,y.shape[1]):\n",
        "          output_pad_mask = get_padding_mask(trg[:,:i])\n",
        "          x,output = tran(x,trg[:,:i],input_pad_mask,output_pad_mask,encoding = enc)\n",
        "          trg[:,i] = output.argmax(-1)[:,-1]\n",
        "          enc = False\n",
        "        target_pad_mask = get_padding_mask(y)\n",
        "        self.test_acc(output.argmax(-1)[target_pad_mask], y[target_pad_mask])\n",
        "        self.log('valid_acc', self.valid_acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "\n",
        "    # def validation_step(self, batch, batch_idx):\n",
        "        # # this is the validation loop\n",
        "        # x, y = batch\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # z = self.encoder(x)\n",
        "        # x_hat = self.decoder(z)\n",
        "        # test_loss = F.mse_loss(x_hat, x)\n",
        "        # self.log(\"val_loss\", test_loss)\n",
        "\n",
        "\n",
        "    def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "      if encoding:\n",
        "        emb = self.embedding(x)\n",
        "        t = (position_embedding(x.shape[0],x.shape[1],self.dmodel,in_padding_mask)).to('cuda')\n",
        "        x = emb + t\n",
        "        x = self.drop1(x)\n",
        "        for i in range(self.nlayers):\n",
        "          x = self.encoders[i](x,in_padding_mask)\n",
        "\n",
        "      emb = self.embedding(z)\n",
        "      t = (position_embedding(z.shape[0],z.shape[1],self.dmodel,out_padding_mask)).to('cuda')\n",
        "      z = emb + t\n",
        "      z = self.drop2(z)\n",
        "      for i in range(self.nlayers):\n",
        "        z = self.decoders[i](z,x,out_padding_mask,in_padding_mask)\n",
        "      \n",
        "      z = z @ self.embedding.E.weight.T\n",
        "\n",
        "      return x,z\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        # this is the test loop\n",
        "        x, y = batch\n",
        "        x = x.view(x.size(0), -1)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return F.softmax(x_hat)"
      ],
      "metadata": {
        "id": "lQDvj4BD7TPX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmodel = 512\n",
        "dk,dv = 64,64\n",
        "nhead = 8\n",
        "df = 2048\n",
        "nlayers = 6\n",
        "vocabulary_size = len(vocab)"
      ],
      "metadata": {
        "id": "K_rQ4Hrr9m9I"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBSzhQRbtGJl",
        "outputId": "736e5ca2-c1e4-4a13-a5d3-90f1d247d108"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tran = Transformer(vocabulary_size,dmodel,df,nhead,nlayers)"
      ],
      "metadata": {
        "id": "rcDDXSJ38CgD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = nn.CrossEntropyLoss()\n",
        "# opt = torch.optim.Adam(tran.parameters(),lr=1e-04, betas=(0.9, 0.98), eps=1e-09)\n",
        "\n",
        "# tran.train()\n",
        "\n",
        "# for i in range(100):\n",
        "    \n",
        "#   opt.zero_grad()\n",
        "#   x,output = tran(input,teacher,in_pad_mask,out_pad_mask)\n",
        "#   pred = output[:,:-1,:].reshape(output.shape[0]*(output.shape[1] - 1),output.shape[2])\n",
        "#   target = teacher[:,1:].reshape(teacher.shape[0]*(teacher.shape[1] - 1))\n",
        "#   l = loss(pred,target)\n",
        "#   l.backward()\n",
        "#   if i%10 == 0 or i == 99:\n",
        "#     print(l)\n",
        "#   #nn.utils.clip_grad_norm_(tran.parameters(), 0.1)\n",
        "#   opt.step()\n",
        "  "
      ],
      "metadata": {
        "id": "7WhL-aIhw81n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output.argmax(-1)[:,:-1]"
      ],
      "metadata": {
        "id": "xgf5xod8CrmV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teacher[:,1:]"
      ],
      "metadata": {
        "id": "LIegSrEkCu8v"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in_test = [\"<sos> ragazzi andiamo a mensa <eos> <pad> <pad>\",\"<sos> il posto fa pagare la compagnia <eos>\"]\n",
        "# out_test = [\"<sos> guys let's go to the canteen <eos>\",\"<sos> the place charges the company <eos> <pad>\"]"
      ],
      "metadata": {
        "id": "np1c4sE_MVXB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_test,in_test_pad_mask = encoding(in_vocabulary,in_test)\n",
        "# target_test,out_test_pad_mask = encoding(out_vocabulary,out_test)"
      ],
      "metadata": {
        "id": "mfuAcZAsMXUg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = input_test\n",
        "# enc = True\n",
        "# trg = target_test.clone()\n",
        "# for i in range(1,target_test.shape[1]):\n",
        "#   x,output = tran(x,trg[:,:i],in_test_pad_mask,out_test_pad_mask[:,:i,:i],encoding = enc)\n",
        "#   trg[:,i] = output.argmax(-1)[:,-1]\n",
        "#   enc = False"
      ],
      "metadata": {
        "id": "pVhluSNDMXtQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output.argmax(-1)"
      ],
      "metadata": {
        "id": "nswprQEINdAR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target_test[:,1:]"
      ],
      "metadata": {
        "id": "k4f7--1lNfZx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# define the logger object \n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"TPTransformer\",log_graph=True )\n",
        "\n",
        "#passing it to the trainer\n",
        "if torch.cuda.is_available() : \n",
        "  trainer = pl.Trainer(gpus=1, max_epochs=2, logger=logger, callbacks=[TQDMProgressBar(refresh_rate=20)])\n",
        "else : trainer = pl.Trainer(gpus=0, max_epochs=2, logger=logger, callbacks=[TQDMProgressBar(refresh_rate=20)])\n",
        "trainer.fit(model=tran, datamodule=dataset)\n",
        "# trainer.validate(datamodule=dataset)\n",
        "# trainer.test(datamodule=dataset)"
      ],
      "metadata": {
        "id": "muqI9nijLoyi",
        "outputId": "4f167b45-2fcc-4ec8-8795-7e3c97895c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "b74bb0183bd649b5bd67240725cbcf10",
            "35d793d7aeb94f5aa097c6625180948a",
            "4a4ad6ead22845f3948c8d03b4368f38",
            "780c361b90e74e31bda65e3c148edaff",
            "2994496e19744e42bf1f2291b83580c5",
            "c54da3b43bda49c5b161541f2aaeb719",
            "f8f34073224a441e94734bfeb4ce04d7",
            "718978e038894727ba3cc5d72548d0a1",
            "27fbdf5b292d42d796f31678039106b9",
            "915d3972336744a4be17ac991da327ef",
            "cdfce29bc659443fa92e06b89eb37715"
          ]
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type               | Params\n",
            "-------------------------------------------------\n",
            "0 | embedding | EmbeddingLayer     | 29.7 K\n",
            "1 | encoders  | ModuleList         | 20.5 M\n",
            "2 | decoders  | ModuleList         | 28.4 M\n",
            "3 | drop1     | Dropout            | 0     \n",
            "4 | drop2     | Dropout            | 0     \n",
            "5 | test_acc  | MulticlassAccuracy | 0     \n",
            "-------------------------------------------------\n",
            "49.0 M    Trainable params\n",
            "0         Non-trainable params\n",
            "49.0 M    Total params\n",
            "195.891   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b74bb0183bd649b5bd67240725cbcf10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ]
    }
  ]
}