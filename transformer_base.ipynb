{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/DL-project/blob/andrea/transformer_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm"
      ],
      "metadata": {
        "id": "6r_zVhYLKhqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(sentences):\n",
        "  vocabulary = {}\n",
        "  for s in sentences:\n",
        "    tokens = s.split()\n",
        "    for t in tokens:\n",
        "      vocabulary[t] = 1\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "6SSXBIoaKEG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_one_hot_encoder(vocabulary):\n",
        "  enc = OneHotEncoder(handle_unknown='ignore')\n",
        "  enc.fit(array(list(vocabulary.keys())).reshape(-1,1))\n",
        "  return enc"
      ],
      "metadata": {
        "id": "Piu4cF7qKbtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(enc,sentences):\n",
        "  X = [[token] for sentence in sentences for token in sentence.split()]\n",
        "  X = torch.tensor(enc.transform(X).todense(),dtype=torch.float32)\n",
        "  X = X.reshape((len(sentences),X.shape[0] // len(sentences),-1))\n",
        "\n",
        "  pad = [1 if token=='<pad>' else 0 for sentence in sentences for token in sentence.split()]\n",
        "  pad = torch.tensor(pad).reshape((X.shape[0],X.shape[1]))\n",
        "  padding_mask = pad.repeat(1,1,X.shape[1]).reshape((X.shape[0],X.shape[1],X.shape[1]))\n",
        "  padding_mask[pad.type(torch.bool)] = 1\n",
        "  padding_mask = padding_mask.type(torch.bool)\n",
        "  \n",
        "  return X,padding_mask"
      ],
      "metadata": {
        "id": "uhBTERsqGOca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_embedding(n_sentences,input_length,dmodel):\n",
        "  res = []\n",
        "  for j in range(n_sentences):\n",
        "    emb = []\n",
        "    for pos in range(input_length):\n",
        "      l = []\n",
        "      for i in range(dmodel):\n",
        "        if i%2 == 0:\n",
        "          l.append(math.sin(pos/10000**(2*i/dmodel)))\n",
        "        else:\n",
        "          l.append(math.cos(pos/10000**(2*i/dmodel)))\n",
        "      emb.append(l)\n",
        "    res.append(emb)\n",
        "  return torch.tensor(res,dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "B8E2HleXTgFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv):\n",
        "    super().__init__()\n",
        "    self.dk = dk\n",
        "    self.Wq = nn.Linear(dmodel,dk)\n",
        "    self.Wk = nn.Linear(dmodel,dk)\n",
        "    self.Wv = nn.Linear(dmodel,dv)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self,x,padding_mask,mask=False,q=None,k=None,v=None,other_mask=None):\n",
        "    if q == None:\n",
        "      q = self.Wq(x)\n",
        "    if k == None:\n",
        "      k = self.Wk(x)\n",
        "    if v == None:\n",
        "      v = self.Wv(x)\n",
        "    sc = torch.matmul(q,k.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "\n",
        "    if other_mask == None:\n",
        "      sc[padding_mask] = float('-inf')\n",
        "    else:\n",
        "      qmod = q.clone()\n",
        "      kmod = k.clone()\n",
        "      qmod[padding_mask[:,0,:]] = 0\n",
        "      kmod[other_mask[:,0,:]] = 0\n",
        "      sc = torch.matmul(qmod,kmod.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "      sc[sc == 0] = float('-inf')\n",
        "\n",
        "    if mask==True:\n",
        "      for i in range(sc.shape[1]):\n",
        "        sc[:,i,i+1:] = float('-inf')\n",
        "    score = torch.matmul(torch.nan_to_num(self.softmax(sc)),v)\n",
        "    return score, q"
      ],
      "metadata": {
        "id": "v9OoIF1MgYIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,nhead,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.nhead = nhead\n",
        "    self.att_layers = nn.ModuleList([SelfAttention(dmodel,dk,dv) for i in range(nhead)])\n",
        "    self.Wo = nn.Linear(dv * nhead, dmodel)\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x,padding_mask,mask=False,q=None,k=None,v=None,other_mask=None):\n",
        "    y = None\n",
        "    q_res = None\n",
        "    if q == None:\n",
        "      y,q_res = self.att_layers[0](x,padding_mask,mask=mask)\n",
        "    else:\n",
        "      y,q_res = self.att_layers[0](x,padding_mask,mask=mask,q=q[:,:,0:dk],k=k,v=v,other_mask=other_mask)\n",
        "    for i in range(1,self.nhead):\n",
        "      if q == None:\n",
        "        y1,q1 = self.att_layers[i](x,padding_mask,mask=mask)\n",
        "        y = torch.cat([y,y1],dim=2)\n",
        "        q_res = torch.cat([q_res,q1],dim=2)\n",
        "      else:\n",
        "        y1,q1 = self.att_layers[i](x,padding_mask,mask=mask,q=q[:,:,i*dk:(i+1)*dk],k=k,v=v,other_mask=other_mask)\n",
        "        y = torch.cat([y,y1],dim=2)\n",
        "    \n",
        "    y = self.Wo(y)\n",
        "    y = self.drop(y)\n",
        "    return y, q_res"
      ],
      "metadata": {
        "id": "wJS-HOR2mYjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,df,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.W1 = nn.Linear(dmodel,df)\n",
        "    self.W2 = nn.Linear(df,dmodel)\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.W1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.W2(x)\n",
        "    x = self.drop(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q3a-LqjhyGER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,df,nhead):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(dmodel,dk,dv,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,padding_mask):\n",
        "    z,_ = self.mha(x,padding_mask)\n",
        "    z = self.norm1(x+z)\n",
        "    y = self.ffn(z)\n",
        "    return self.norm2(z+y)\n"
      ],
      "metadata": {
        "id": "mIPuC2RAypA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,df,nhead):\n",
        "    super().__init__()\n",
        "    self.masked_mha = MultiHeadAttention(dmodel,dk,dv,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.enc_dec_attention = MultiHeadAttention(dmodel,dk,dv,nhead)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm3 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,q,k,v,padding_mask,other_mask):\n",
        "    z1,q_res = self.masked_mha(x,padding_mask,mask=True)\n",
        "    z1 = self.norm1(x+z1)\n",
        "    z2,_ = self.enc_dec_attention(z1,padding_mask,mask=False,q=q_res,k=k,v=v,other_mask=other_mask)\n",
        "    z2 = self.norm2(z1+z2)\n",
        "    y = self.ffn(z2)\n",
        "    return self.norm3(z2+y)\n"
      ],
      "metadata": {
        "id": "DLqzXKv6kRua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,in_voc_size,out_voc_size,dmodel,dk,dv,df,nhead,nlayers,input_size=1):\n",
        "    super().__init__()\n",
        "    self.nlayers = nlayers\n",
        "    self.in_embedding = nn.Parameter(torch.randn(in_voc_size,dmodel))\n",
        "    self.out_embedding = nn.Parameter(torch.randn(out_voc_size,dmodel))\n",
        "    self.encoders = nn.ModuleList([Encoder(dmodel,dk,dv,df,nhead) for i in range(nlayers)])\n",
        "    self.decoders = nn.ModuleList([Decoder(dmodel,dk,dv,df,nhead) for i in range(nlayers)])\n",
        "    self.Wk = nn.Linear(dmodel,dk)\n",
        "    self.Wv = nn.Linear(dmodel,dv)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "    if encoding:\n",
        "      emb = (x @ self.in_embedding) * math.sqrt(dmodel)\n",
        "      t = position_embedding(x.shape[0],x.shape[1],dmodel)\n",
        "      x = emb + t\n",
        "      for i in range(self.nlayers):\n",
        "        x = self.encoders[i](x,in_padding_mask)\n",
        "    \n",
        "    Kenc = self.Wk(x)\n",
        "    Venc = self.Wv(x)\n",
        "\n",
        "    emb = (z @ self.out_embedding) * math.sqrt(dmodel)\n",
        "    t = position_embedding(z.shape[0],z.shape[1],dmodel)\n",
        "    z = emb + t\n",
        "    for i in range(self.nlayers):\n",
        "      z = self.decoders[i](z,None,Kenc,Venc,out_padding_mask,in_padding_mask)\n",
        "    \n",
        "    z = z @ self.out_embedding.T\n",
        "    z = self.softmax(z)\n",
        "\n",
        "    return x,z"
      ],
      "metadata": {
        "id": "0lB1U2Ee4UCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_sentences = [\"<sos> dai ragazzi per una volta che ci andiamo non scegliamo il posto che fa pagare poco <eos>\",\"<sos> altrimenti tanto vale andare a mensa <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\",\"<sos> importante è la compagnia <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n",
        "in_vocabulary = create_vocabulary(in_sentences)\n",
        "in_enc = create_one_hot_encoder(in_vocabulary)"
      ],
      "metadata": {
        "id": "qFlLPZH78meU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_sentences = [\"<sos> come on guys for once let's not choose the place that charges little <eos>\",\"<sos> otherwise we might as well go to the canteen <eos> <pad> <pad> <pad> <pad>\",\"<sos> important is company <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n",
        "out_vocabulary = create_vocabulary(out_sentences)\n",
        "out_enc = create_one_hot_encoder(out_vocabulary)"
      ],
      "metadata": {
        "id": "Z6oWxTJqkbRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in2_sentences = [\"dai ragazzi per una volta che ci andiamo non scegliamo il posto che fa pagare poco <eos>\",\"altrimenti tanto vale andare a mensa <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\",\"importante è la compagnia <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n",
        "out2_sentences = [\"<sos> come on guys for once let's not choose the place that charges little\",\"<sos> otherwise we might as well go to the canteen <pad> <pad> <pad> <pad>\",\"<sos> important is company <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n"
      ],
      "metadata": {
        "id": "F58TxV7ut_9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input,in_pad_mask = one_hot_encoding(in_enc,in2_sentences)\n",
        "teacher,out_pad_mask = one_hot_encoding(out_enc,out2_sentences)"
      ],
      "metadata": {
        "id": "TcuxIU-xLZd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmodel = 512\n",
        "dk,dv = 64,64\n",
        "nhead = 8\n",
        "df = 2048\n",
        "nlayers = 6\n",
        "in_vocabulary_size = len(in_vocabulary.keys())\n",
        "out_vocabulary_size = len(out_vocabulary.keys())"
      ],
      "metadata": {
        "id": "K_rQ4Hrr9m9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tran = Transformer(in_vocabulary_size,out_vocabulary_size,dmodel,dk,dv,df,nhead,nlayers)"
      ],
      "metadata": {
        "id": "rcDDXSJ38CgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,output = tran(input,teacher,in_pad_mask,out_pad_mask)"
      ],
      "metadata": {
        "id": "c2gkaNNlLvgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out3_sentences = [\"come on guys for once let's not choose the place that charges little <eos>\",\"otherwise we might as well go to the canteen <eos> <pad> <pad> <pad> <pad>\",\"important is company <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n",
        "target,target_pad_mask = one_hot_encoding(out_enc,out3_sentences)"
      ],
      "metadata": {
        "id": "7c0n1EXCuq43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(tran.parameters(),lr=1e-04, betas=(0.9, 0.98), eps=1e-09)\n",
        "\n",
        "tran.train()\n",
        "\n",
        "for i in range(50):\n",
        "    \n",
        "  opt.zero_grad()\n",
        "  x,output = tran(input,teacher,in_pad_mask,out_pad_mask)\n",
        "  l = loss(output, target)\n",
        "  l.backward()\n",
        "  #nn.utils.clip_grad_norm_(tran.parameters(), 0.1)\n",
        "  opt.step()"
      ],
      "metadata": {
        "id": "7WhL-aIhw81n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}