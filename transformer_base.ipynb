{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/DL-project/blob/main/transformer_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm"
      ],
      "metadata": {
        "id": "6r_zVhYLKhqs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(sentences):\n",
        "  vocabulary = {}\n",
        "  vocabulary['<sos>'] = 1\n",
        "  vocabulary['<eos>'] = 2\n",
        "  vocabulary['<pad>'] = 0\n",
        "  index = 3\n",
        "  for s in sentences:\n",
        "    tokens = s.split()\n",
        "    for t in tokens:\n",
        "      if t not in vocabulary:\n",
        "        vocabulary[t] = index\n",
        "        index += 1\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "6SSXBIoaKEG8"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_padding_mask(vocabulary,X):\n",
        "  pad = X == vocabulary['<pad>']\n",
        "  padding_mask = pad.repeat(1,1,X.shape[1]).reshape((X.shape[0],X.shape[1],X.shape[1]))\n",
        "  padding_mask[pad] = True\n",
        "  return padding_mask"
      ],
      "metadata": {
        "id": "UHx-Jf5baYoX"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(vocabulary,sentences):\n",
        "  X = [[vocabulary[token]] for sentence in sentences for token in sentence.split()]\n",
        "  X = torch.tensor(X)\n",
        "  X = X.reshape((len(sentences),X.shape[0] // len(sentences)))\n",
        "  padding_mask = get_padding_mask(vocabulary,X)\n",
        "  return X,padding_mask"
      ],
      "metadata": {
        "id": "uhBTERsqGOca"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "  def __init__(self,vocabulary_size,embedding_dim):\n",
        "    super().__init__()\n",
        "    self.E = nn.Embedding(vocabulary_size,embedding_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.E(x)"
      ],
      "metadata": {
        "id": "C4i4vs8nLKNz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_embedding(batch_size,seq_length,emb_dim):\n",
        "  res = torch.zeros((batch_size,seq_length,emb_dim),dtype=torch.float32)\n",
        "  for pos in range(seq_length):\n",
        "    for i in range(emb_dim):\n",
        "      if i%2 == 0:\n",
        "        res[:,pos,i] = math.sin(pos/10000**(2*i/emb_dim))\n",
        "      else:\n",
        "        res[:,pos,i] = math.cos(pos/10000**(2*i/emb_dim))\n",
        "  return res"
      ],
      "metadata": {
        "id": "B8E2HleXTgFz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv):\n",
        "    super().__init__()\n",
        "    self.dk = dk\n",
        "    self.Wq = nn.Linear(dmodel,dk)\n",
        "    self.Wk = nn.Linear(dmodel,dk)\n",
        "    self.Wv = nn.Linear(dmodel,dv)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    q = self.Wq(x)\n",
        "    if enc == None:\n",
        "      k = self.Wk(x)\n",
        "      v = self.Wv(x)\n",
        "    else:\n",
        "      k = self.Wk(enc)\n",
        "      v = self.Wv(enc)\n",
        "    sc = torch.matmul(q,k.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "\n",
        "    if other_mask == None:\n",
        "      sc[padding_mask] = float('-inf')\n",
        "    else:\n",
        "      qmod = q.clone()\n",
        "      kmod = k.clone()\n",
        "      qmod[padding_mask[:,0,:]] = 0\n",
        "      kmod[other_mask[:,0,:]] = 0\n",
        "      sc = torch.matmul(qmod,kmod.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "      sc[sc == 0] = float('-inf')\n",
        "\n",
        "    if mask==True:\n",
        "      for i in range(sc.shape[1]):\n",
        "        sc[:,i,i+1:] = float('-inf')\n",
        "    score = torch.matmul(torch.nan_to_num(self.softmax(sc)),v)\n",
        "    return score"
      ],
      "metadata": {
        "id": "v9OoIF1MgYIw"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,nhead,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.nhead = nhead\n",
        "    self.att_layers = nn.ModuleList([SelfAttention(dmodel,dk,dv) for i in range(nhead)])\n",
        "    self.Wo = nn.Linear(dv * nhead, dmodel)\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    y = self.att_layers[0](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    for i in range(1,self.nhead):\n",
        "      y = torch.cat([y,self.att_layers[i](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)],dim=2)\n",
        "    y = self.Wo(y)\n",
        "    y = self.drop(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "wJS-HOR2mYjA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,df,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.W1 = nn.Linear(dmodel,df)\n",
        "    self.W2 = nn.Linear(df,dmodel)\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.W1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.W2(x)\n",
        "    x = self.drop(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q3a-LqjhyGER"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,df,nhead):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(dmodel,dk,dv,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,padding_mask):\n",
        "    z = self.mha(x,padding_mask)\n",
        "    z = self.norm1(x+z)\n",
        "    y = self.ffn(z)\n",
        "    return self.norm2(z+y)"
      ],
      "metadata": {
        "id": "mIPuC2RAypA5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,df,nhead):\n",
        "    super().__init__()\n",
        "    self.masked_mha = MultiHeadAttention(dmodel,dk,dv,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.enc_dec_attention = MultiHeadAttention(dmodel,dk,dv,nhead)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm3 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,enc,padding_mask,other_mask):\n",
        "    z1= self.masked_mha(x,padding_mask,mask=True)\n",
        "    z1 = self.norm1(x+z1)\n",
        "    z2= self.enc_dec_attention(z1,padding_mask,enc=enc,mask=False,other_mask=other_mask)\n",
        "    z2 = self.norm2(z1+z2)\n",
        "    y = self.ffn(z2)\n",
        "    return self.norm3(z2+y)"
      ],
      "metadata": {
        "id": "DLqzXKv6kRua"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,voc_size,dmodel,dk,dv,df,nhead,nlayers,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.nlayers = nlayers\n",
        "    self.embedding = EmbeddingLayer(voc_size,dmodel)\n",
        "    self.encoders = nn.ModuleList([Encoder(dmodel,dk,dv,df,nhead) for i in range(nlayers)])\n",
        "    self.decoders = nn.ModuleList([Decoder(dmodel,dk,dv,df,nhead) for i in range(nlayers)])\n",
        "    self.drop1 = nn.Dropout(p=dropout)\n",
        "    self.drop2 = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "    if encoding:\n",
        "      emb = self.embedding(x) * math.sqrt(dmodel)\n",
        "      t = position_embedding(x.shape[0],x.shape[1],dmodel)\n",
        "      x = emb + t\n",
        "      x = self.drop1(x)\n",
        "      for i in range(self.nlayers):\n",
        "        x = self.encoders[i](x,in_padding_mask)\n",
        "\n",
        "    emb = self.embedding(z) * math.sqrt(dmodel)\n",
        "    t = position_embedding(z.shape[0],z.shape[1],dmodel)\n",
        "    z = emb + t\n",
        "    z = self.drop2(z)\n",
        "    for i in range(self.nlayers):\n",
        "      z = self.decoders[i](z,x,out_padding_mask,in_padding_mask)\n",
        "    \n",
        "    z = z @ self.embedding.E.weight.T\n",
        "    z[out_padding_mask[:,0,:]] = 0\n",
        "\n",
        "    return x,z"
      ],
      "metadata": {
        "id": "0lB1U2Ee4UCh"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_sentences = [\"<sos> dai ragazzi per una volta che ci andiamo non scegliamo il posto che fa pagare poco <eos>\",\"<sos> altrimenti tanto vale andare a mensa <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\",\"<sos> importante Ã¨ la compagnia <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n",
        "in_vocabulary = create_vocabulary(in_sentences)"
      ],
      "metadata": {
        "id": "qFlLPZH78meU"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_sentences = [\"<sos> come on guys for once let's not choose the place that charges little <eos>\",\"<sos> otherwise we might as well go to the canteen <eos> <pad> <pad> <pad> <pad>\",\"<sos> important is company <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"]\n",
        "out_vocabulary = create_vocabulary(out_sentences)"
      ],
      "metadata": {
        "id": "Z6oWxTJqkbRu"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input,in_pad_mask = encoding(in_vocabulary,in_sentences)\n",
        "teacher,out_pad_mask = encoding(out_vocabulary,out_sentences)"
      ],
      "metadata": {
        "id": "TcuxIU-xLZd5"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmodel = 512\n",
        "dk,dv = 64,64\n",
        "nhead = 8\n",
        "df = 2048\n",
        "nlayers = 6\n",
        "in_vocabulary_size = len(in_vocabulary.keys())\n",
        "out_vocabulary_size = len(out_vocabulary.keys())"
      ],
      "metadata": {
        "id": "K_rQ4Hrr9m9I"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tran = Transformer(in_vocabulary_size,dmodel,dk,dv,df,nhead,nlayers)"
      ],
      "metadata": {
        "id": "rcDDXSJ38CgD"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(tran.parameters(),lr=1e-04, betas=(0.9, 0.98), eps=1e-09)\n",
        "\n",
        "tran.train()\n",
        "\n",
        "for i in range(100):\n",
        "    \n",
        "  opt.zero_grad()\n",
        "  x,output = tran(input,teacher,in_pad_mask,out_pad_mask)\n",
        "  pred = output[:,:-1,:].reshape(output.shape[0]*(output.shape[1] - 1),output.shape[2])\n",
        "  target = teacher[:,1:].reshape(teacher.shape[0]*(teacher.shape[1] - 1))\n",
        "  l = loss(pred,target)\n",
        "  l.backward()\n",
        "  if i%10 == 0 or i == 99:\n",
        "    print(l)\n",
        "  #nn.utils.clip_grad_norm_(tran.parameters(), 0.1)\n",
        "  opt.step()\n",
        "  "
      ],
      "metadata": {
        "id": "7WhL-aIhw81n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57ee504-c341-490d-8455-5afc61ec77dd"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(215.1943, grad_fn=<NllLossBackward0>)\n",
            "tensor(14.6847, grad_fn=<NllLossBackward0>)\n",
            "tensor(8.2658, grad_fn=<NllLossBackward0>)\n",
            "tensor(8.0056, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.9022, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1587, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1726, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7645, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7326, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.5739, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1091, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.argmax(-1)[:,:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuIKfBfOVe2l",
        "outputId": "2adda855-2d4e-4bc5-b626-03976b735610"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 15],\n",
              "        [16, 17, 18, 19, 20, 21, 22, 11, 23,  2,  0,  0,  0,  0],\n",
              "        [24, 25, 26,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher[:,1:]"
      ],
      "metadata": {
        "id": "LIegSrEkCu8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a733c47f-bd68-4a50-cb52-58bd2d35f606"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  2],\n",
              "        [16, 17, 18, 19, 20, 21, 22, 11, 23,  2,  0,  0,  0,  0],\n",
              "        [24, 25, 26,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_test = [\"<sos> ragazzi andiamo a mensa <eos> <pad> <pad>\",\"<sos> il posto fa pagare la compagnia <eos>\"]\n",
        "out_test = [\"<sos> guys let's go to the canteen <eos>\",\"<sos> the place charges the company <eos> <pad>\"]"
      ],
      "metadata": {
        "id": "np1c4sE_MVXB"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_test,in_test_pad_mask = encoding(in_vocabulary,in_test)\n",
        "target_test,out_test_pad_mask = encoding(out_vocabulary,out_test)"
      ],
      "metadata": {
        "id": "mfuAcZAsMXUg"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = input_test\n",
        "enc = True\n",
        "trg = target_test.clone()\n",
        "tran.eval()\n",
        "for i in range(1,target_test.shape[1]):\n",
        "  output_pad_mask = get_padding_mask(out_vocabulary,trg[:,:i])\n",
        "  x,output = tran(x,trg[:,:i],in_test_pad_mask,output_pad_mask,encoding = enc)\n",
        "  trg[:,i] = output.argmax(-1)[:,-1]\n",
        "  enc = False"
      ],
      "metadata": {
        "id": "pVhluSNDMXtQ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.argmax(-1)"
      ],
      "metadata": {
        "id": "nswprQEINdAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093cda95-31f0-4419-ca40-74bfb9ddcd7e"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3,  4,  5,  6,  7,  8,  9],\n",
              "        [24, 25, 26,  2,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_test[:,1:]"
      ],
      "metadata": {
        "id": "k4f7--1lNfZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1ef1e7-5811-4bc6-88eb-c9ea76974cf5"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  8, 21, 22, 11, 23,  2],\n",
              "        [11, 12, 14, 11, 26,  2,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    }
  ]
}