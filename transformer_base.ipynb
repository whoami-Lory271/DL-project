{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76629a17daa24bff9887a0a38dd49001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b03c46c8da141858f6b5d5d9384b628",
              "IPY_MODEL_7a668ce16863486ead339202e2287e54",
              "IPY_MODEL_4b273b6368f04dd7a280e535590d3cb4"
            ],
            "layout": "IPY_MODEL_702cba1ef0f84d08a983c18242e1686c"
          }
        },
        "7b03c46c8da141858f6b5d5d9384b628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bceb0363953e425fb905ddf878e07010",
            "placeholder": "​",
            "style": "IPY_MODEL_ccd57d1827cb4c0d9a9d785a6e5a7103",
            "value": "Epoch 0:   0%"
          }
        },
        "7a668ce16863486ead339202e2287e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb82b229674462f90b706ffb083607e",
            "max": 15625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e48bdbf93ff45eb9474741310e4b07d",
            "value": 0
          }
        },
        "4b273b6368f04dd7a280e535590d3cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6859f9252862443fbe409473b2415955",
            "placeholder": "​",
            "style": "IPY_MODEL_5f510f09efd14e17a5fe97931f496a3b",
            "value": " 0/15625 [00:00&lt;?, ?it/s]"
          }
        },
        "702cba1ef0f84d08a983c18242e1686c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "bceb0363953e425fb905ddf878e07010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd57d1827cb4c0d9a9d785a6e5a7103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eb82b229674462f90b706ffb083607e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e48bdbf93ff45eb9474741310e4b07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6859f9252862443fbe409473b2415955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f510f09efd14e17a5fe97931f496a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/DL-project/blob/main/transformer_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install library import "
      ],
      "metadata": {
        "id": "EYE_g4aS0itt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install torchmetrics --quiet"
      ],
      "metadata": {
        "id": "q7IXEB390pTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import pickle\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar"
      ],
      "metadata": {
        "id": "6r_zVhYLKhqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "egcxUIj5C4dr",
        "outputId": "634cfe58-ad51-46a4-ba5f-495bd58a9af9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function"
      ],
      "metadata": {
        "id": "ojG0bbT6C-JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_padding_mask(X):\n",
        "  pad = X == 0\n",
        "  padding_mask = pad.repeat(1,1,X.shape[1]).reshape((X.shape[0],X.shape[1],X.shape[1]))\n",
        "  padding_mask[pad] = True\n",
        "  return padding_mask"
      ],
      "metadata": {
        "id": "oZ4yDtvtGMSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding(encode, vocab, max_len):\n",
        "    enc = np.array(encode)\n",
        "    encoded = np.zeros(max_len + 2)\n",
        "    for k in range(len(enc)):\n",
        "      encoded[k] = enc[k]\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "C4JVsg3-t_RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "j2OkY9ii2qjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Deep learning\" #change if needed"
      ],
      "metadata": {
        "id": "DE4BW2JmUeSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for easy train data:"
      ],
      "metadata": {
        "id": "05HMjuQ8AkRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#take all the txt files\n",
        "easy_path = os.path.join(save_path,\"easy_pickle\")\n",
        "easy_texts_path=[str(path) for path in Path(easy_path).glob(\"*.txt\")]\n",
        "print(easy_texts_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pVI-CFneMDR",
        "outputId": "aa569b66-3b97-4937-d2ab-1fff8e1e2bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/easy_pickle/algebra__linear_1d_easy.txt', '/content/drive/MyDrive/Deep learning/easy_pickle/calculus__differentiate_easy.txt', '/content/drive/MyDrive/Deep learning/easy_pickle/probability__swr_p_level_set_easy.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for medium train data:"
      ],
      "metadata": {
        "id": "gq78jqGTAnmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medium_path= os.path.join(save_path,\"medium_pickle\")\n",
        "medium_texts_path=[str(path) for path in Path(medium_path).glob(\"*.txt\")]\n",
        "print(medium_texts_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lVIhd7-eMzJ",
        "outputId": "30d5c853-12a1-45de-a4e5-e8f23fc3d526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/medium_pickle/algebra__linear_1d_medium.txt', '/content/drive/MyDrive/Deep learning/medium_pickle/calculus__differentiate_medium.txt', '/content/drive/MyDrive/Deep learning/medium_pickle/probability__swr_p_level_set_medium.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for hard train data:"
      ],
      "metadata": {
        "id": "0K8p7SMEApjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hard_path= os.path.join(save_path,\"hard_pickle\")\n",
        "hard_texts_path=[str(path) for path in Path(hard_path).glob(\"*.txt\")]\n",
        "print(hard_texts_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7pos2G7eMsR",
        "outputId": "8cb85480-ddf4-4ce6-9a08-64fe08b8bf19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/hard_pickle/algebra__linear_1d_hard.txt', '/content/drive/MyDrive/Deep learning/hard_pickle/calculus__differentiate_hard.txt', '/content/drive/MyDrive/Deep learning/hard_pickle/probability__swr_p_level_set_hard.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for interpolate data:"
      ],
      "metadata": {
        "id": "_cDGRuQdAxE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_path= os.path.join(save_path,\"interpolate_pickle\")\n",
        "interpolate_texts_path=[str(path) for path in Path(interpolate_path).glob(\"*.txt\")]\n",
        "print(interpolate_texts_path)"
      ],
      "metadata": {
        "id": "Q2vkjK2BAuNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary_from_directory(sentences_path):\n",
        "  vocabulary = {}\n",
        "  vocabulary['<sos>'] = 1\n",
        "  vocabulary['<eos>'] = 2\n",
        "  vocabulary['<pad>'] = 0\n",
        "  index = 3\n",
        "  conta = 0\n",
        "  for file in sentences_path:\n",
        "    with open(file, 'r') as file_in:\n",
        "      text = file_in.read()\n",
        "      sentences = text.split('\\n')\n",
        "      sentences_new = sentences[:-1] #remove last element because it's empty\n",
        "      print(\"num sentences\", len(sentences_new))\n",
        "      for s in sentences_new:\n",
        "        conta +=1\n",
        "        for t in s:\n",
        "         if t not in vocabulary:\n",
        "            vocabulary[t] = index\n",
        "            index += 1\n",
        "      file_in.close()\n",
        "  return vocabulary, conta"
      ],
      "metadata": {
        "id": "NenOz7Bnz1gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary, conta = create_vocabulary_from_directory(ft_texts_path)\n",
        "# print(conta)"
      ],
      "metadata": {
        "id": "gw7vYDuiy5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #create the pickle\n",
        "# outfile = open(os.path.join(save_path,\"vocabs\"), 'wb')\n",
        "# pickle.dump(vocabulary, outfile)\n",
        "# outfile.close()"
      ],
      "metadata": {
        "id": "hozVF8z6KqDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_from_sentence(vocab, sentence):\n",
        "  enc= [1]\n",
        "  for token in sentence:\n",
        "    enc.append(vocab[token])\n",
        "  enc.append(2)\n",
        "  return enc"
      ],
      "metadata": {
        "id": "yE-32CsOX3en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pickles(vocabulary, sentences_paths, column_names=[\"Encoding\"], path_quest=\"/content/drive/MyDrive/Deep learning/questions\", path_answ=\"/content/drive/MyDrive/Deep learning/answers\"):\n",
        "  quest = []\n",
        "  answ = []\n",
        "  out_file_quest = open(path_quest, 'wb')\n",
        "  out_file_answ = open(path_answ, 'wb')\n",
        "  for file in sentences_paths:\n",
        "    with open(file, 'r') as file_in:\n",
        "      text = file_in.read()\n",
        "      sentences = text.split('\\n')\n",
        "      sentences = sentences[:-1]\n",
        "      print(\"num sentences\", len(sentences))\n",
        "      for i, sentence in enumerate(sentences):\n",
        "          enc = encoding_from_sentence(vocabulary, sentence)\n",
        "          if i % 2 == 0: #question\n",
        "            #quest.append({\"Sentence\": sentence, \"Encoding\": enc})\n",
        "            quest.append({\"Encoding\": enc})\n",
        "          elif i % 2 == 1: #answer\n",
        "            #answ.append({\"Sentence\": sentence, \"Encoding\": enc})\n",
        "            answ.append({\"Encoding\": enc})\n",
        "  pickle.dump(quest, out_file_quest)\n",
        "  pickle.dump(answ, out_file_answ)\n",
        "  out_file_quest.close()\n",
        "  out_file_answ.close()\n",
        "  return path_quest, path_answ"
      ],
      "metadata": {
        "id": "VJbw05t8Mjdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "C8mQUo-4MPvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the vocabulary\n",
        "infile = open(os.path.join(save_path,'vocabs'),'rb')\n",
        "vocab = pickle.load(infile)"
      ],
      "metadata": {
        "id": "eJKBOht4EPHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want to train easy data:"
      ],
      "metadata": {
        "id": "rb7cgdXyA7eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(easy_path,\"questions\"),'rb'):\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(easy_path,\"answers\"),'rb'):\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoXn82htefTr",
        "outputId": "b02a5e96-d907-4115-d8a4-bb721966a69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999998\n",
            "1999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want to train medium data:"
      ],
      "metadata": {
        "id": "RYcpWDaYA---"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(medium_path,\"questions\"),'rb'):\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(medium_path,\"answers\"),'rb')\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "id": "t8e47VRR05pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want to train hard data:"
      ],
      "metadata": {
        "id": "tFdhkvjIBGdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(hard_path,\"questions\"),'rb'):\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(hard_path,\"answers\"),'rb'):\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "id": "VugmdRjuBJSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MathematicsDataset(Dataset):\n",
        "  def __init__(self, quest, answ, vocab , max_len_quest=160, max_len_answ=30):\n",
        "    self.quest = quest\n",
        "    self.answ = answ\n",
        "    self.vocab = vocab\n",
        "    self.max_len_quest = max_len_quest\n",
        "    self.max_len_answ = max_len_answ\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.quest)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    assert(idx  < len(self.quest)) #indices should start from 0 to len - 1 (there are 5999994 elements)\n",
        "    question = self.quest[idx][\"Encoding\"]\n",
        "    encoding1 = torch.from_numpy(create_padding(question, self.vocab, self.max_len_quest))\n",
        "    encoding1 = encoding1.type(torch.int64)\n",
        "    #self.sample[\"Question\"] = encoding\n",
        "      \n",
        "    answer = self.answ[idx][\"Encoding\"]\n",
        "    encoding2 = torch.from_numpy(create_padding(answer, self.vocab, self.max_len_answ))\n",
        "    encoding2 = encoding2.type(torch.int64)\n",
        "    #self.sample[\"Answer\"] = encoding\n",
        "      \n",
        "    return encoding1, encoding2 "
      ],
      "metadata": {
        "id": "bCskAS9LMNTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlMathematicsDataset(pl.LightningDataModule):\n",
        "    def __init__(self, train_quest, train_answ, vocab , test_quest, test_answ, max_len_quest=160, max_len_answ=30, batch_size=128):\n",
        "        super().__init__()\n",
        "        self.train_quest = train_quest\n",
        "        self.train_answ = train_answ\n",
        "        self.test_quest = test_quest\n",
        "        self.test_answ = test_answ\n",
        "        self.vocab = vocab\n",
        "        self.max_len_quest = max_len_quest\n",
        "        self.max_len_answ = max_len_answ\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = MathematicsDataset(self.train_quest, self.train_answ, self.vocab)\n",
        "\n",
        "    # def prepare_data(self):\n",
        "        # tok, load, ecc...\n",
        "        # MNIST(self.data_dir, train=True, download=True)\n",
        "        # MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_ds = MathematicsDataset(self.train_quest, self.train_answ, self.vocab)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_ds = MathematicsDataset(self.test_quest, self.test_answ, self.vocab)\n",
        "\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            self.predict_ds = MathematicsDataset(self.test_quest, self.test_answ, self.vocab)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle = True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle = True)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.predict_ds, batch_size=self.batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "dXLqCNPH1ZNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dataset\n",
        "dataset = PlMathematicsDataset(quest, answ, vocab, quest, answ)"
      ],
      "metadata": {
        "id": "sAQcoCkOo44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Create dataloader\n",
        "# train_loader = DataLoader(dataset, 32, shuffle=True)\n",
        "# print(len(train_loader))"
      ],
      "metadata": {
        "id": "03Zzt0OmDgbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"data\", next(iter(train_loader)))\n",
        "# print(\"questions_shape \", next(iter(train_loader))[0].shape)\n",
        "\n",
        "# print(\"answers_shape \", next(iter(train_loader))[1].shape)"
      ],
      "metadata": {
        "id": "uWqFcYLND4ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Kf5YD-k_z5H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "  def __init__(self,vocabulary_size,embedding_dim):\n",
        "    super().__init__()\n",
        "    self.E = nn.Embedding(vocabulary_size,embedding_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.E(x)"
      ],
      "metadata": {
        "id": "C4i4vs8nLKNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_embedding(batch_size,seq_length,emb_dim,pad_mask):\n",
        "  res = torch.zeros((batch_size,seq_length,emb_dim),dtype=torch.float32)\n",
        "  for pos in range(seq_length):\n",
        "    for i in range(emb_dim):\n",
        "      if i%2 == 0:\n",
        "        res[:,pos,i] = math.sin(pos/10000**(2*i/emb_dim))\n",
        "      else:\n",
        "        res[:,pos,i] = math.cos(pos/10000**(2*i/emb_dim))\n",
        "  res[pad_mask[:,0,:]] = 0\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  return res.to(device)"
      ],
      "metadata": {
        "id": "B8E2HleXTgFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPSelfAttention(nn.Module):\n",
        "  def __init__(self,dz,dk):\n",
        "    super().__init__()\n",
        "    self.dk = dk\n",
        "    self.Wq = nn.Linear(dz,dk)\n",
        "    self.Wk = nn.Linear(dz,dk)\n",
        "    self.Wv = nn.Linear(dz,dk)\n",
        "    self.Wr = nn.Linear(dz,dk)\n",
        "    self.Wo = nn.Linear(dk,dz)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    q = self.Wq(x)\n",
        "    r = self.Wr(x)\n",
        "    if enc == None:\n",
        "      k = self.Wk(x)\n",
        "      v = self.Wv(x)\n",
        "    else:\n",
        "      k = self.Wk(enc)\n",
        "      v = self.Wv(enc)\n",
        "    sc = torch.matmul(q,k.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "\n",
        "    if other_mask == None:\n",
        "      sc[padding_mask] = float('-inf')\n",
        "    else:\n",
        "      qmod = q.clone()\n",
        "      kmod = k.clone()\n",
        "      qmod[padding_mask[:,0,:]] = 0\n",
        "      kmod[other_mask[:,0,:]] = 0\n",
        "      sc = torch.matmul(qmod,kmod.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "      sc[sc == 0] = float('-inf')\n",
        "\n",
        "    if mask==True:\n",
        "      for i in range(sc.shape[1]):\n",
        "        sc[:,i,i+1:] = float('-inf')\n",
        "    score = torch.matmul(torch.nan_to_num(self.softmax(sc)),v)\n",
        "    return self.Wo(r * score)"
      ],
      "metadata": {
        "id": "v9OoIF1MgYIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,dmodel,nhead,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.nhead = nhead\n",
        "    self.att_layers = nn.ModuleList([TPSelfAttention(dmodel,dmodel // nhead) for i in range(nhead)])\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    y = self.att_layers[0](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    for i in range(1,self.nhead):\n",
        "      y += self.att_layers[i](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    y = self.drop(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "wJS-HOR2mYjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,df,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.W1 = nn.Linear(dmodel,df)\n",
        "    self.W2 = nn.Linear(df,dmodel)\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.W1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.W2(x)\n",
        "    x = self.drop(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q3a-LqjhyGER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dmodel,df,nhead):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(dmodel)\n",
        "    self.mha = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,padding_mask):\n",
        "    x = self.norm(x)\n",
        "    z = self.mha(x,padding_mask)\n",
        "    z = self.norm1(x+z)\n",
        "    y = self.ffn(z)\n",
        "    return self.norm2(z+y)"
      ],
      "metadata": {
        "id": "mIPuC2RAypA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dmodel,df,nhead):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(dmodel)\n",
        "    self.masked_mha = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.enc_dec_attention = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm3 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,enc,padding_mask,other_mask):\n",
        "    x = self.norm(x)\n",
        "    z1= self.masked_mha(x,padding_mask,mask=True)\n",
        "    z1 = self.norm1(x+z1)\n",
        "    z2= self.enc_dec_attention(z1,padding_mask,enc=enc,mask=False,other_mask=other_mask)\n",
        "    z2 = self.norm2(z1+z2)\n",
        "    y = self.ffn(z2)\n",
        "    return self.norm3(z2+y)"
      ],
      "metadata": {
        "id": "DLqzXKv6kRua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Transformer(nn.Module):\n",
        "#   def __init__(self,voc_size,dmodel,df,nhead,nlayers,dropout=0.1):\n",
        "#     super().__init__()\n",
        "#     self.nlayers = nlayers\n",
        "#     self.embedding = EmbeddingLayer(voc_size,dmodel)\n",
        "#     self.encoders = nn.ModuleList([Encoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "#     self.decoders = nn.ModuleList([Decoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "#     self.drop1 = nn.Dropout(p=dropout)\n",
        "#     self.drop2 = nn.Dropout(p=dropout)\n",
        "\n",
        "#   def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "#     if encoding:\n",
        "#       emb = self.embedding(x)\n",
        "#       t = position_embedding(x.shape[0],x.shape[1],dmodel,in_padding_mask)\n",
        "#       x = emb + t\n",
        "#       x = self.drop1(x)\n",
        "#       for i in range(self.nlayers):\n",
        "#         x = self.encoders[i](x,in_padding_mask)\n",
        "\n",
        "#     emb = self.embedding(z)\n",
        "#     t = position_embedding(z.shape[0],z.shape[1],dmodel,out_padding_mask)\n",
        "#     z = emb + t\n",
        "#     z = self.drop2(z)\n",
        "#     for i in range(self.nlayers):\n",
        "#       z = self.decoders[i](z,x,out_padding_mask,in_padding_mask)\n",
        "    \n",
        "#     z = z @ self.embedding.E.weight.T\n",
        "\n",
        "#     return x,z"
      ],
      "metadata": {
        "id": "0lB1U2Ee4UCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(pl.LightningModule):\n",
        "    def __init__(self,voc_size,dmodel,df,nhead,nlayers,dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.nlayers = nlayers\n",
        "      self.embedding = EmbeddingLayer(voc_size,dmodel)\n",
        "      self.encoders = nn.ModuleList([Encoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "      self.decoders = nn.ModuleList([Decoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "      self.drop1 = nn.Dropout(p=dropout)\n",
        "      self.drop2 = nn.Dropout(p=dropout)\n",
        "      self.dmodel = dmodel\n",
        "      self.metric = nn.CrossEntropyLoss()\n",
        "      self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes = voc_size)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        input, teacher = batch\n",
        "        in_pad_mask = get_padding_mask(input)\n",
        "        out_pad_mask = get_padding_mask(teacher)\n",
        "        x,output = tran(input,teacher,in_pad_mask,out_pad_mask)\n",
        "        pred = output[:,:-1,:].reshape(output.shape[0]*(output.shape[1] - 1),output.shape[2])\n",
        "        target = teacher[:,1:].reshape(teacher.shape[0]*(teacher.shape[1] - 1))\n",
        "        loss = self.metric(pred,target)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(tran.parameters(),lr=1e-04, betas=(0.9, 0.995), eps=1e-09)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x,y = batch\n",
        "        enc = True\n",
        "        trg = y.clone()\n",
        "        input_pad_mask = get_padding_mask(x)\n",
        "        for i in range(1,y.shape[1]):\n",
        "          output_pad_mask = get_padding_mask(trg[:,:i])\n",
        "          x,output = tran(x,trg[:,:i],input_pad_mask,output_pad_mask,encoding = enc)\n",
        "          trg[:,i] = output.argmax(-1)[:,-1]\n",
        "          enc = False\n",
        "        target_pad_mask = get_padding_mask(y[:,1:])\n",
        "        self.test_acc(output.argmax(-1)[target_pad_mask[:,0,:]], y[:,1:][target_pad_mask[:,0,:]])\n",
        "        self.log('test_acc', self.test_acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "      if encoding:\n",
        "        emb = self.embedding(x)\n",
        "        t = position_embedding(x.shape[0],x.shape[1],self.dmodel,in_padding_mask)\n",
        "        x = emb + t\n",
        "        x = self.drop1(x)\n",
        "        for i in range(self.nlayers):\n",
        "          x = self.encoders[i](x,in_padding_mask)\n",
        "\n",
        "      emb = self.embedding(z)\n",
        "      t = position_embedding(z.shape[0],z.shape[1],self.dmodel,out_padding_mask)\n",
        "      z = emb + t\n",
        "      z = self.drop2(z)\n",
        "      for i in range(self.nlayers):\n",
        "        z = self.decoders[i](z,x,out_padding_mask,in_padding_mask)\n",
        "      \n",
        "      z = z @ self.embedding.E.weight.T\n",
        "      z[out_padding_mask[:,0,:]] = 0\n",
        "\n",
        "      return x,z\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x,y = batch\n",
        "        enc = True\n",
        "        trg = y.clone()\n",
        "        input_pad_mask = get_padding_mask(x)\n",
        "        for i in range(1,y.shape[1]):\n",
        "          output_pad_mask = get_padding_mask(trg[:,:i])\n",
        "          x,output = tran(x,trg[:,:i],input_pad_mask,output_pad_mask,encoding = enc)\n",
        "          trg[:,i] = output.argmax(-1)[:,-1]\n",
        "          enc = False\n",
        "        return output.argmax(-1)"
      ],
      "metadata": {
        "id": "lQDvj4BD7TPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmodel = 512\n",
        "dk,dv = 64,64\n",
        "nhead = 8\n",
        "df = 512\n",
        "nlayers = 6\n",
        "vocabulary_size = len(vocab)\n",
        "enable_checkpointing = True\n",
        "ckpt_path = None #\"/content/drive/MyDrive/Deep learning/model_checkpoints/last.ckpt\""
      ],
      "metadata": {
        "id": "K_rQ4Hrr9m9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tran = Transformer(vocabulary_size,dmodel,df,nhead,nlayers)"
      ],
      "metadata": {
        "id": "rcDDXSJ38CgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# define the logger object \n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"TPTransformer\",log_graph=True )\n",
        "\n",
        "#passing it to the trainer\n",
        "if torch.cuda.is_available() : \n",
        "  trainer = pl.Trainer(default_root_dir=\"/content/drive/MyDrive/Deep learning/model_checkpoints\", enable_checkpointing=enable_checkpointing, gpus=1, max_epochs=2, logger=logger, callbacks=[TQDMProgressBar(refresh_rate=20)])\n",
        "else : trainer = pl.Trainer(default_root_dir=\"/content/drive/MyDrive/Deep learning/model_checkpoints\", enable_checkpointing=enable_checkpointing ,gpus=0, max_epochs=2, logger=logger, callbacks=[TQDMProgressBar(refresh_rate=20)])\n",
        "trainer.fit(ckpt_path=ckpt_path, model=tran, datamodule=dataset)"
      ],
      "metadata": {
        "id": "muqI9nijLoyi",
        "outputId": "1a14f73e-e582-447c-b8f3-7bb8716dcfc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "76629a17daa24bff9887a0a38dd49001",
            "7b03c46c8da141858f6b5d5d9384b628",
            "7a668ce16863486ead339202e2287e54",
            "4b273b6368f04dd7a280e535590d3cb4",
            "702cba1ef0f84d08a983c18242e1686c",
            "bceb0363953e425fb905ddf878e07010",
            "ccd57d1827cb4c0d9a9d785a6e5a7103",
            "0eb82b229674462f90b706ffb083607e",
            "6e48bdbf93ff45eb9474741310e4b07d",
            "6859f9252862443fbe409473b2415955",
            "5f510f09efd14e17a5fe97931f496a3b"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: tb_logs/TPTransformer\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type               | Params\n",
            "-------------------------------------------------\n",
            "0 | embedding | EmbeddingLayer     | 29.7 K\n",
            "1 | encoders  | ModuleList         | 11.1 M\n",
            "2 | decoders  | ModuleList         | 19.0 M\n",
            "3 | drop1     | Dropout            | 0     \n",
            "4 | drop2     | Dropout            | 0     \n",
            "5 | metric    | CrossEntropyLoss   | 0     \n",
            "6 | test_acc  | MulticlassAccuracy | 0     \n",
            "-------------------------------------------------\n",
            "30.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "30.1 M    Total params\n",
            "120.320   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76629a17daa24bff9887a0a38dd49001"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want interpolate data:"
      ],
      "metadata": {
        "id": "MUQgviypBUeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(interpolate_path,\"questions\"),'rb') as infile:\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(interpolate_path,\"answers\"),'rb') as infile:\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "id": "F4EQ4DGVBZac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(tran,datamodule=dataset)"
      ],
      "metadata": {
        "id": "nxsrenIn-Lri"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}