{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "405868fcdbac434199903b85c7fde8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b62615cad0ec4bab8a5f41aaea752616",
              "IPY_MODEL_ced1f641e49a4aa18591431c919efe59",
              "IPY_MODEL_eaab3edb5e5f484a80c603998f4e0490"
            ],
            "layout": "IPY_MODEL_b5e1f054608b40f7a2c813cd98e05666"
          }
        },
        "b62615cad0ec4bab8a5f41aaea752616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc23c5df5a294bf5bafb93af2c9ee650",
            "placeholder": "​",
            "style": "IPY_MODEL_29d42aaf7448437cadfdeae57c35c14c",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "ced1f641e49a4aa18591431c919efe59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a41f55d1e422472685e03264914b88e9",
            "max": 59,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e85cb7320a574545afa910584b499ec1",
            "value": 59
          }
        },
        "eaab3edb5e5f484a80c603998f4e0490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f803c600cdc4518a5fb7fab8fcb28d6",
            "placeholder": "​",
            "style": "IPY_MODEL_c1b345a6d74341859ca7a484193c8cdb",
            "value": " 59/59 [05:55&lt;00:00,  6.03s/it]"
          }
        },
        "b5e1f054608b40f7a2c813cd98e05666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "fc23c5df5a294bf5bafb93af2c9ee650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d42aaf7448437cadfdeae57c35c14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a41f55d1e422472685e03264914b88e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e85cb7320a574545afa910584b499ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f803c600cdc4518a5fb7fab8fcb28d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1b345a6d74341859ca7a484193c8cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/DL-project/blob/main/TPTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install library import "
      ],
      "metadata": {
        "id": "EYE_g4aS0itt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "q7IXEB390pTj",
        "outputId": "24aa7af4-588e-4036-9b94-6efa3286cb2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/825.8 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/825.8 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/512.4 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.4.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a62a18f765d3e179174d9b40280cbda3b23d26151f42c9b99d9ab4fbc1e7ab97\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, GitPython, wandb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.30 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.14.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.14 wandb-0.13.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import pickle\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar"
      ],
      "metadata": {
        "id": "6r_zVhYLKhqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "egcxUIj5C4dr",
        "outputId": "ba32a64a-0e77-4a97-c796-62767c134fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function"
      ],
      "metadata": {
        "id": "ojG0bbT6C-JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_padding_mask(X):\n",
        "  pad = X == 0\n",
        "  padding_mask = pad.repeat(1,1,X.shape[1]).reshape((X.shape[0],X.shape[1],X.shape[1]))\n",
        "  padding_mask[pad] = True\n",
        "  return padding_mask"
      ],
      "metadata": {
        "id": "oZ4yDtvtGMSC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding(encode, vocab, max_len):\n",
        "    enc = np.array(encode)\n",
        "    encoded = np.zeros(max_len + 2)\n",
        "    for k in range(len(enc)):\n",
        "      encoded[k] = enc[k]\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "C4JVsg3-t_RN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "j2OkY9ii2qjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Deep learning\" #change if needed"
      ],
      "metadata": {
        "id": "DE4BW2JmUeSB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for easy train data:"
      ],
      "metadata": {
        "id": "05HMjuQ8AkRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#take all the txt files\n",
        "easy_path = os.path.join(save_path,\"easy_pickle\")\n",
        "easy_texts_path=[str(path) for path in Path(easy_path).glob(\"*.txt\")]\n",
        "print(easy_texts_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pVI-CFneMDR",
        "outputId": "68534fd3-8c54-4252-b8f6-b7f2c806e2f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/easy_pickle/algebra__linear_1d_easy.txt', '/content/drive/MyDrive/Deep learning/easy_pickle/calculus__differentiate_easy.txt', '/content/drive/MyDrive/Deep learning/easy_pickle/probability__swr_p_level_set_easy.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for medium train data:"
      ],
      "metadata": {
        "id": "gq78jqGTAnmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medium_path= os.path.join(save_path,\"medium_pickle\")\n",
        "medium_texts_path=[str(path) for path in Path(medium_path).glob(\"*.txt\")]\n",
        "print(medium_texts_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lVIhd7-eMzJ",
        "outputId": "7af19a99-a30f-4155-dd02-05c04e3cd21c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/medium_pickle/algebra__linear_1d_medium.txt', '/content/drive/MyDrive/Deep learning/medium_pickle/calculus__differentiate_medium.txt', '/content/drive/MyDrive/Deep learning/medium_pickle/probability__swr_p_level_set_medium.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for hard train data:"
      ],
      "metadata": {
        "id": "0K8p7SMEApjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hard_path= os.path.join(save_path,\"hard_pickle\")\n",
        "hard_texts_path=[str(path) for path in Path(hard_path).glob(\"*.txt\")]\n",
        "print(hard_texts_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7pos2G7eMsR",
        "outputId": "a9805abb-c8c4-4a4f-9a71-925dcebec617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/hard_pickle/algebra__linear_1d_hard.txt', '/content/drive/MyDrive/Deep learning/hard_pickle/calculus__differentiate_hard.txt', '/content/drive/MyDrive/Deep learning/hard_pickle/probability__swr_p_level_set_hard.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path for interpolate data:"
      ],
      "metadata": {
        "id": "_cDGRuQdAxE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_path= os.path.join(save_path,\"interpolate_pickle\")\n",
        "interpolate_texts_path=[str(path) for path in Path(interpolate_path).glob(\"*.txt\")]\n",
        "print(interpolate_texts_path)"
      ],
      "metadata": {
        "id": "Q2vkjK2BAuNg",
        "outputId": "204eee0a-b612-4a61-c4b5-16a314b783b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Deep learning/interpolate_pickle/algebra__linear_1d.txt', '/content/drive/MyDrive/Deep learning/interpolate_pickle/calculus__differentiate.txt', '/content/drive/MyDrive/Deep learning/interpolate_pickle/probability__swr_p_level_set.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary_from_directory(sentences_path):\n",
        "  vocabulary = {}\n",
        "  vocabulary['<sos>'] = 1\n",
        "  vocabulary['<eos>'] = 2\n",
        "  vocabulary['<pad>'] = 0\n",
        "  index = 3\n",
        "  conta = 0\n",
        "  for file in sentences_path:\n",
        "    with open(file, 'r') as file_in:\n",
        "      text = file_in.read()\n",
        "      sentences = text.split('\\n')\n",
        "      sentences_new = sentences[:-1] #remove last element because it's empty\n",
        "      print(\"num sentences\", len(sentences_new))\n",
        "      for s in sentences_new:\n",
        "        conta +=1\n",
        "        for t in s:\n",
        "         if t not in vocabulary:\n",
        "            vocabulary[t] = index\n",
        "            index += 1\n",
        "      file_in.close()\n",
        "  return vocabulary, conta"
      ],
      "metadata": {
        "id": "NenOz7Bnz1gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary, conta = create_vocabulary_from_directory(ft_texts_path)\n",
        "# print(conta)"
      ],
      "metadata": {
        "id": "gw7vYDuiy5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #create the pickle\n",
        "# outfile = open(os.path.join(save_path,\"vocabs\"), 'wb')\n",
        "# pickle.dump(vocabulary, outfile)\n",
        "# outfile.close()"
      ],
      "metadata": {
        "id": "hozVF8z6KqDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_from_sentence(vocab, sentence):\n",
        "  enc= [1]\n",
        "  for token in sentence:\n",
        "    enc.append(vocab[token])\n",
        "  enc.append(2)\n",
        "  return enc"
      ],
      "metadata": {
        "id": "yE-32CsOX3en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pickles(vocabulary, sentences_paths, column_names=[\"Encoding\"], path_quest=\"/content/drive/MyDrive/Deep learning/questions\", path_answ=\"/content/drive/MyDrive/Deep learning/answers\"):\n",
        "  quest = []\n",
        "  answ = []\n",
        "  out_file_quest = open(path_quest, 'wb')\n",
        "  out_file_answ = open(path_answ, 'wb')\n",
        "  for file in sentences_paths:\n",
        "    with open(file, 'r') as file_in:\n",
        "      text = file_in.read()\n",
        "      sentences = text.split('\\n')\n",
        "      sentences = sentences[:-1]\n",
        "      print(\"num sentences\", len(sentences))\n",
        "      for i, sentence in enumerate(sentences):\n",
        "          enc = encoding_from_sentence(vocabulary, sentence)\n",
        "          if i % 2 == 0: #question\n",
        "            #quest.append({\"Sentence\": sentence, \"Encoding\": enc})\n",
        "            quest.append({\"Encoding\": enc})\n",
        "          elif i % 2 == 1: #answer\n",
        "            #answ.append({\"Sentence\": sentence, \"Encoding\": enc})\n",
        "            answ.append({\"Encoding\": enc})\n",
        "  pickle.dump(quest, out_file_quest)\n",
        "  pickle.dump(answ, out_file_answ)\n",
        "  out_file_quest.close()\n",
        "  out_file_answ.close()\n",
        "  return path_quest, path_answ"
      ],
      "metadata": {
        "id": "VJbw05t8Mjdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "C8mQUo-4MPvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the vocabulary\n",
        "infile = open(os.path.join(save_path,'vocabs'),'rb')\n",
        "vocab = pickle.load(infile)\n",
        "infile.close()"
      ],
      "metadata": {
        "id": "eJKBOht4EPHd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want to train easy data:"
      ],
      "metadata": {
        "id": "rb7cgdXyA7eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(easy_path,\"questions\"),'rb') as infile:\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(easy_path,\"answers\"),'rb') as infile:\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoXn82htefTr",
        "outputId": "d9cb00f9-7dbb-4b53-ad19-2ea9153fa08f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999998\n",
            "1999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want to train medium data:"
      ],
      "metadata": {
        "id": "RYcpWDaYA---"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(medium_path,\"questions\"),'rb') as infile:\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(medium_path,\"answers\"),'rb') as infile:\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "id": "t8e47VRR05pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c336b1ba-d364-4e23-b128-56096dad411e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999998\n",
            "1999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want to train hard data:"
      ],
      "metadata": {
        "id": "tFdhkvjIBGdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(hard_path,\"questions\"),'rb') as infile:\n",
        "  quest = pickle.load(infile)\n",
        "print(len(quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(hard_path,\"answers\"),'rb') as infile:\n",
        "  answ = pickle.load(infile)\n",
        "print(len(answ))"
      ],
      "metadata": {
        "id": "VugmdRjuBJSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute to load test:"
      ],
      "metadata": {
        "id": "9JUXVoRZmh6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickle the qestions\n",
        "with open(os.path.join(interpolate_path,\"questions\"),'rb') as infile:\n",
        "  test_quest = pickle.load(infile)\n",
        "print(len(test_quest))\n",
        "\n",
        "#unpickle the answers\n",
        "with open(os.path.join(interpolate_path,\"answers\"),'rb') as infile:\n",
        "  test_answ = pickle.load(infile)\n",
        "print(len(test_answ))"
      ],
      "metadata": {
        "id": "F4EQ4DGVBZac",
        "outputId": "a13013c8-a02e-4ee4-e2cc-9624b9a834b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MathematicsDataset(Dataset):\n",
        "  def __init__(self, quest, answ, vocab , max_len_quest=160, max_len_answ=30):\n",
        "    self.quest = quest\n",
        "    self.answ = answ\n",
        "    self.vocab = vocab\n",
        "    self.max_len_quest = max_len_quest\n",
        "    self.max_len_answ = max_len_answ\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.quest)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    assert(idx  < len(self.quest)) #indices should start from 0 to len - 1 (there are 5999994 elements)\n",
        "    question = self.quest[idx][\"Encoding\"]\n",
        "    encoding1 = torch.from_numpy(create_padding(question, self.vocab, self.max_len_quest))\n",
        "    encoding1 = encoding1.type(torch.int64)\n",
        "    #self.sample[\"Question\"] = encoding\n",
        "      \n",
        "    answer = self.answ[idx][\"Encoding\"]\n",
        "    encoding2 = torch.from_numpy(create_padding(answer, self.vocab, self.max_len_answ))\n",
        "    encoding2 = encoding2.type(torch.int64)\n",
        "    #self.sample[\"Answer\"] = encoding\n",
        "      \n",
        "    return encoding1, encoding2 "
      ],
      "metadata": {
        "id": "bCskAS9LMNTT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlMathematicsDataset(pl.LightningDataModule):\n",
        "    def __init__(self, train_quest, train_answ, vocab , test_quest, test_answ, max_len_quest=160, max_len_answ=30, batch_size=512):\n",
        "        super().__init__()\n",
        "        self.train_quest = train_quest\n",
        "        self.train_answ = train_answ\n",
        "        self.test_quest = test_quest\n",
        "        self.test_answ = test_answ\n",
        "        self.vocab = vocab\n",
        "        self.max_len_quest = max_len_quest\n",
        "        self.max_len_answ = max_len_answ\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = MathematicsDataset(self.train_quest, self.train_answ, self.vocab)\n",
        "\n",
        "    # def prepare_data(self):\n",
        "        # tok, load, ecc...\n",
        "        # MNIST(self.data_dir, train=True, download=True)\n",
        "        # MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_ds = MathematicsDataset(self.train_quest, self.train_answ, self.vocab)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_ds = MathematicsDataset(self.test_quest, self.test_answ, self.vocab)\n",
        "\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            self.predict_ds = MathematicsDataset(self.test_quest, self.test_answ, self.vocab)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle = True, num_workers=12)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_ds, batch_size=self.batch_size)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.predict_ds, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "dXLqCNPH1ZNQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dataset\n",
        "dataset = PlMathematicsDataset(quest, answ, vocab, test_quest, test_answ)"
      ],
      "metadata": {
        "id": "sAQcoCkOo44i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Create dataloader\n",
        "# train_loader = DataLoader(dataset, 32, shuffle=True)\n",
        "# print(len(train_loader))"
      ],
      "metadata": {
        "id": "03Zzt0OmDgbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"data\", next(iter(train_loader)))\n",
        "# print(\"questions_shape \", next(iter(train_loader))[0].shape)\n",
        "\n",
        "# print(\"answers_shape \", next(iter(train_loader))[1].shape)"
      ],
      "metadata": {
        "id": "uWqFcYLND4ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Kf5YD-k_z5H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "  def __init__(self,vocabulary_size,embedding_dim):\n",
        "    super().__init__()\n",
        "    self.E = nn.Embedding(vocabulary_size,embedding_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.E(x)"
      ],
      "metadata": {
        "id": "C4i4vs8nLKNz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def position_embedding(batch_size,seq_length,emb_dim,pad_mask):\n",
        "  res = torch.zeros((batch_size,seq_length,emb_dim),dtype=torch.float32)\n",
        "  for pos in range(seq_length):\n",
        "    for i in range(emb_dim):\n",
        "      if i%2 == 0:\n",
        "        res[:,pos,i] = math.sin(pos/10000**(2*i/emb_dim))\n",
        "      else:\n",
        "        res[:,pos,i] = math.cos(pos/10000**(2*i/emb_dim))\n",
        "  res[pad_mask[:,0,:]] = 0\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  return res.to(device)"
      ],
      "metadata": {
        "id": "B8E2HleXTgFz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPSelfAttention(nn.Module):\n",
        "  def __init__(self,dz,dk):\n",
        "    super().__init__()\n",
        "    self.dk = dk\n",
        "    self.Wq = nn.Linear(dz,dk)\n",
        "    self.Wk = nn.Linear(dz,dk)\n",
        "    self.Wv = nn.Linear(dz,dk)\n",
        "    self.Wr = nn.Linear(dz,dk)\n",
        "    self.Wo = nn.Linear(dk,dz)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    q = self.Wq(x)\n",
        "    r = self.Wr(x)\n",
        "    if enc == None:\n",
        "      k = self.Wk(x)\n",
        "      v = self.Wv(x)\n",
        "    else:\n",
        "      k = self.Wk(enc)\n",
        "      v = self.Wv(enc)\n",
        "    sc = torch.matmul(q,k.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "\n",
        "    if other_mask == None:\n",
        "      sc[padding_mask] = float('-inf')\n",
        "    else:\n",
        "      qmod = q.clone()\n",
        "      kmod = k.clone()\n",
        "      qmod[padding_mask[:,0,:]] = 0\n",
        "      kmod[other_mask[:,0,:]] = 0\n",
        "      sc = torch.matmul(qmod,kmod.permute(0,2,1)) / math.sqrt(self.dk)\n",
        "      sc[sc == 0] = float('-inf')\n",
        "\n",
        "    if mask==True:\n",
        "      for i in range(sc.shape[1]):\n",
        "        sc[:,i,i+1:] = float('-inf')\n",
        "    score = torch.matmul(torch.nan_to_num(self.softmax(sc)),v)\n",
        "    return self.Wo(r * score)"
      ],
      "metadata": {
        "id": "v9OoIF1MgYIw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,dmodel,nhead,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.nhead = nhead\n",
        "    self.att_layers = nn.ModuleList([TPSelfAttention(dmodel,dmodel // nhead) for i in range(nhead)])\n",
        "    #self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x,padding_mask,enc=None,mask=False,other_mask=None):\n",
        "    y = self.att_layers[0](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    for i in range(1,self.nhead):\n",
        "      y += self.att_layers[i](x,padding_mask,enc=enc,mask=mask,other_mask=other_mask)\n",
        "    #y = self.drop(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "wJS-HOR2mYjA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,df,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.W1 = nn.Linear(dmodel,df)\n",
        "    self.W2 = nn.Linear(df,dmodel)\n",
        "    #self.drop = nn.Dropout(p=dropout)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.W1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.W2(x)\n",
        "    #x = self.drop(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Q3a-LqjhyGER"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dmodel,df,nhead):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(dmodel)\n",
        "    self.mha = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,padding_mask):\n",
        "    x = self.norm(x)\n",
        "    z = self.mha(x,padding_mask)\n",
        "    z = self.norm1(x+z)\n",
        "    y = self.ffn(z)\n",
        "    return self.norm2(z+y)"
      ],
      "metadata": {
        "id": "mIPuC2RAypA5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dmodel,df,nhead):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(dmodel)\n",
        "    self.masked_mha = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm1 = LayerNorm(dmodel)\n",
        "    self.enc_dec_attention = TPMultiHeadAttention(dmodel,nhead)\n",
        "    self.norm2 = LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,df)\n",
        "    self.norm3 = LayerNorm(dmodel)\n",
        "  \n",
        "  def forward(self,x,enc,padding_mask,other_mask):\n",
        "    x = self.norm(x)\n",
        "    z1= self.masked_mha(x,padding_mask,mask=True)\n",
        "    z1 = self.norm1(x+z1)\n",
        "    z2= self.enc_dec_attention(z1,padding_mask,enc=enc,mask=False,other_mask=other_mask)\n",
        "    z2 = self.norm2(z1+z2)\n",
        "    y = self.ffn(z2)\n",
        "    return self.norm3(z2+y)"
      ],
      "metadata": {
        "id": "DLqzXKv6kRua"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPTransformer(pl.LightningModule):\n",
        "    def __init__(self,voc_size,dmodel,df,nhead,nlayers,dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.save_hyperparameters()\n",
        "      self.nlayers = nlayers\n",
        "      self.embedding = EmbeddingLayer(voc_size,dmodel)\n",
        "      self.encoders = nn.ModuleList([Encoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "      self.decoders = nn.ModuleList([Decoder(dmodel,df,nhead) for i in range(nlayers)])\n",
        "      #self.drop1 = nn.Dropout(p=dropout)\n",
        "      #self.drop2 = nn.Dropout(p=dropout)\n",
        "      self.dmodel = dmodel\n",
        "      self.metric = nn.CrossEntropyLoss()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defines the train loop.\n",
        "        input, teacher = batch\n",
        "        in_pad_mask = get_padding_mask(input)\n",
        "        out_pad_mask = get_padding_mask(teacher)\n",
        "        x,output = tran(input,teacher,in_pad_mask,out_pad_mask)\n",
        "        pred = output[:,:-1,:].reshape(output.shape[0]*(output.shape[1] - 1),output.shape[2])\n",
        "        target = teacher[:,1:].reshape(teacher.shape[0]*(teacher.shape[1] - 1))\n",
        "        loss = self.metric(pred,target)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(tran.parameters(),lr=1e-04, betas=(0.9, 0.995), eps=1e-09)\n",
        "        return optimizer\n",
        "    \n",
        "    def accuracy(self,pred,targ):\n",
        "      c = 0\n",
        "      for i in range(targ.shape[0]):\n",
        "        if targ[i] == pred[i]:\n",
        "          c += 1\n",
        "      return c\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x,y = batch\n",
        "        enc = True\n",
        "        trg = y.clone()\n",
        "        input_pad_mask = get_padding_mask(x)\n",
        "        for i in range(1,y.shape[1]):\n",
        "          output_pad_mask = get_padding_mask(trg[:,:i])\n",
        "          x,output = tran(x,trg[:,:i],input_pad_mask,output_pad_mask,encoding = enc)\n",
        "          trg[:,i] = output.argmax(-1)[:,-1]\n",
        "          enc = False\n",
        "        target_pad_mask = get_padding_mask(y[:,1:])\n",
        "        acc = self.accuracy(output.argmax(-1)[target_pad_mask[:,0,:]], y[:,1:][target_pad_mask[:,0,:]])\n",
        "        self.log('test_acc', acc)\n",
        "        return acc\n",
        "\n",
        "    def forward(self,x,z,in_padding_mask,out_padding_mask,encoding=True):\n",
        "      if encoding:\n",
        "        emb = self.embedding(x)\n",
        "        t = position_embedding(x.shape[0],x.shape[1],self.dmodel,in_padding_mask)\n",
        "        x = emb + t\n",
        "        #x = self.drop1(x)\n",
        "        for i in range(self.nlayers):\n",
        "          x = self.encoders[i](x,in_padding_mask)\n",
        "\n",
        "      emb = self.embedding(z)\n",
        "      t = position_embedding(z.shape[0],z.shape[1],self.dmodel,out_padding_mask)\n",
        "      z = emb + t\n",
        "      #z = self.drop2(z)\n",
        "      for i in range(self.nlayers):\n",
        "        z = self.decoders[i](z,x,out_padding_mask,in_padding_mask)\n",
        "      \n",
        "      z = z @ self.embedding.E.weight.T\n",
        "\n",
        "      return x,z\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x,y = batch\n",
        "        enc = True\n",
        "        trg = y.clone()\n",
        "        input_pad_mask = get_padding_mask(x)\n",
        "        for i in range(1,y.shape[1]):\n",
        "          output_pad_mask = get_padding_mask(trg[:,:i])\n",
        "          x,output = tran(x,trg[:,:i],input_pad_mask,output_pad_mask,encoding = enc)\n",
        "          trg[:,i] = output.argmax(-1)[:,-1]\n",
        "          enc = False\n",
        "        return output.argmax(-1)"
      ],
      "metadata": {
        "id": "lQDvj4BD7TPX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['WANDB_API_KEY'] = 'ec7e306bca4079bed1f89da133ac2940aa0e531c'"
      ],
      "metadata": {
        "id": "XvytRv4ADwOF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmodel = 512\n",
        "dk,dv = 64,64\n",
        "nhead = 8\n",
        "df = 512\n",
        "nlayers = 6\n",
        "vocabulary_size = len(vocab)\n",
        "enable_checkpointing = True\n",
        "resume_training = True\n",
        "load_model = True\n",
        "load_model_dir = \"/content/drive/MyDrive/Deep learning/model_checkpoints/TPTransformer/last.ckpt\" #change path to load a different checkpoint"
      ],
      "metadata": {
        "id": "K_rQ4Hrr9m9I"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tran = TPTransformer(vocabulary_size,dmodel,df,nhead,nlayers)\n",
        "if load_model == True:\n",
        "  tran = TPTransformer.load_from_checkpoint(load_model_dir)"
      ],
      "metadata": {
        "id": "rcDDXSJ38CgD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(project='DeepLearning')\n",
        "wandb.watch(tran,log_freq=10)\n",
        "wandb_logger = WandbLogger(project='DeepLearning')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "p2GxMFShEFiB",
        "outputId": "17980022-1380-4f05-8ecd-7344d6e42135"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcelestini-1845289\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230130_192529-mxd8qvr8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/celestini-1845289/DeepLearning/runs/mxd8qvr8\" target=\"_blank\">scintillating-goat-5</a></strong> to <a href=\"https://wandb.ai/celestini-1845289/DeepLearning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/celestini-1845289/DeepLearning\" target=\"_blank\">https://wandb.ai/celestini-1845289/DeepLearning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/celestini-1845289/DeepLearning/runs/mxd8qvr8\" target=\"_blank\">https://wandb.ai/celestini-1845289/DeepLearning/runs/mxd8qvr8</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=\"/content/drive/MyDrive/Deep learning/model_checkpoints/TPTransformer\", save_last =True, save_on_train_epoch_end = True)\n",
        "\n",
        "#build the correct callbacks\n",
        "callbacks = [TQDMProgressBar(refresh_rate=20)]\n",
        "ckpt_path = load_model_dir\n",
        "\n",
        "#handle checkpointing\n",
        "if resume_training == False:\n",
        "  ckpt_path = None\n",
        "if enable_checkpointing == True:\n",
        "  callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "  \n",
        "#passing it to the trainer\n",
        "gpu = 0\n",
        "if torch.cuda.is_available() : \n",
        "  gpu = 1\n",
        "trainer = pl.Trainer(enable_checkpointing=enable_checkpointing ,gpus=gpu, max_epochs=2, logger=wandb_logger, callbacks=callbacks)\n",
        "#trainer.fit(ckpt_path=ckpt_path, model=tran, datamodule=dataset)\n",
        "print(checkpoint_callback.best_model_path)"
      ],
      "metadata": {
        "id": "muqI9nijLoyi",
        "outputId": "451802d9-c894-40a2-8de1-a7aa73380c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute if you want interpolate data:"
      ],
      "metadata": {
        "id": "MUQgviypBUeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(tran,datamodule=dataset)"
      ],
      "metadata": {
        "id": "nxsrenIn-Lri",
        "outputId": "3436f59d-09d0-4e5d-dd5a-e93f90529f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "405868fcdbac434199903b85c7fde8dc",
            "b62615cad0ec4bab8a5f41aaea752616",
            "ced1f641e49a4aa18591431c919efe59",
            "eaab3edb5e5f484a80c603998f4e0490",
            "b5e1f054608b40f7a2c813cd98e05666",
            "fc23c5df5a294bf5bafb93af2c9ee650",
            "29d42aaf7448437cadfdeae57c35c14c",
            "a41f55d1e422472685e03264914b88e9",
            "e85cb7320a574545afa910584b499ec1",
            "4f803c600cdc4518a5fb7fab8fcb28d6",
            "c1b345a6d74341859ca7a484193c8cdb"
          ]
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.accelerators.cuda:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "405868fcdbac434199903b85c7fde8dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('test_acc', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test_acc                10997.9375\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_acc': 10997.9375}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "uhgqvh_aE23Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}